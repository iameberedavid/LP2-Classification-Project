{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TELCO CHURN ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "Null Hypothesis: There is no relationship between the monthly charges and the churn of customers.\n",
    "\n",
    "Alternate Hypothesis: There is a relationship between the monthly charges and the churn of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Questions\n",
    "1. What is the overall churn rate of the telecommunication company?\n",
    "2. What is the average monthly charges to churn customers compared to non-churn customers?\n",
    "3. What percentage of the top 100 most charged customers churned?\n",
    "4. What percentage of the top 100 least charged customers churned?\n",
    "5. What is the churn rate of male customers with partners, dependents and high monthly charges?\n",
    "6. What is the churn rate of customers without online security?\n",
    "7. What is the churn rate of customers without online backup?\n",
    "8. What is the churn rate of customers without device protection?\n",
    "9. What is the churn rate of customers without Tech support?\n",
    "10. How does the absence of online security, online backup, device protection and Tech support add up to lead to churning?\n",
    "11. How does the length of customers' contract affect their likelihood of churn?\n",
    "12. How does the length of customers' tenure affect their likelihood of churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 980,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Libraries to create connection string to SQL server\n",
    "import pyodbc\n",
    "from dotenv import dotenv_values\n",
    "\n",
    "# Libraries for visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Library for testing the hypothesis\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Library for splitting the train data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Library for feature scaling\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Library for feature encoding\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Libraries for balancing the dataset\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "# Libraries for modelling\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Libraries for evaluation\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three datasets for this analysis which are located in three different places. The first dataset can be found on a SQL server and is to be accessed remotely. The second dataset can be found as an excel file on One Drive. The access link was provided and used to access and download the dataset. While the third dataset can be found as a csv file on a Github repository whose link was provided as well and used to clone the dataset into the local machine.\n",
    "\n",
    "The first and last datasets have been identified as the train datasets, while the second dataset has been identifed as the test dataset. The train datasets will be assessed and merged together, and used to build models independently. While the test dataset will be used to test the models independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the first dataset from SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 981,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the environment variable in the .env file into a dictionary\n",
    "\n",
    "environment_variables = dotenv_values('.env')\n",
    "\n",
    "# Get the values for the credentials you set in the .env file\n",
    "server = environment_variables.get(\"SERVER\")\n",
    "database = environment_variables.get(\"DATABASE\")\n",
    "username = environment_variables.get(\"USERNAME\")\n",
    "password = environment_variables.get(\"PASSWORD\")\n",
    "\n",
    "# The connection string is an f string that includes all the variable above to extablish a connection to the server.\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 982,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the connect method of the pyodbc library to pass in the connection string.\n",
    "\n",
    "# N/B: This will connect to the server and might take a few seconds to be complete.\n",
    "# Check your internet connection if it takes more time than necessary.\n",
    "\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 983,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the dataset using the SQL query shown below\n",
    "# dbo.LP2_Telco_churn_first_3000 is the name of the dataset, dbo being a naming convention in Microsoft SQL Server.\n",
    "\n",
    "query = \"Select * from dbo.LP2_Telco_churn_first_3000\"\n",
    "df1 = pd.read_sql(query, connection)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the last dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 984,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the last dataset.\n",
    "\n",
    "df3 = pd.read_csv('LP2_Telco-churn-last-2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 985,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first five rows of the last dataset.\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 986,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the columns of the first dataset.\n",
    "\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 987,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the columns of the last dataset.\n",
    "\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 988,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since both datasets have the same column names and index number, they can be concatenated to have the train dataset.\n",
    "\n",
    "train_data = pd.concat([df1, df3])\n",
    "\n",
    "# Saving the train dataset to a new csv file.\n",
    "train_data.to_csv('Train-Data.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the second dataset (test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 989,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test dataset.\n",
    "\n",
    "test_data = pd.read_excel('Telco-churn-second-2000.xlsx')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 990,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of rows and columns on the train dataset.\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 991,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of rows and columns on the test dataset.\n",
    "\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset has 5043 columns and 21 columns while the test dataset has 2000 rows and 20 columns. Let's identify the column in the train dataset that is absent in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 992,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the columns of the train dataset.\n",
    "\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 993,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the columns of the test dataset.\n",
    "\n",
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the test dataset has the same columns as the train dataset with the exception of the churn column which can be found on the train dataset alone. This is understandable as the churn column on the train dataset provides information on whether customers churn or  not, which is used to build the best ML model. This column is not needed on the test dataset, rather the model built is tested on the test dataset to check the ability of the model to predict whether a customet will churn or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatypes and the presence of missing values on the train dataset.\n",
    "\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatypes and the presence of missing values on the test dataset.\n",
    "\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the datatype of the 'SeniorCitizen' column is an object on the train data but an integer on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 996,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the number of cells with missing values on each column of the train dataset.\n",
    "\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'MultipleLines' column has 269 missing values. The 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV' and 'StreamingMovies' columns all have 651 missing values. This needs to be evaluated further to find out if these missing values are exactly on the same rows. The 'TotalCharges' column has 5 missing values while the 'Churn' column has 1 missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 997,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming that there are no missing values on the test dataset.\n",
    "\n",
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 998,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the presence of duplicates on the train dataset.\n",
    "\n",
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows on the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 999,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the presence of duplicates on the test dataset.\n",
    "\n",
    "test_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the columns on the train dataset.\n",
    "\n",
    "train_data.columns\n",
    "for column in train_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, train_data[column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are many coloumns with unnecessary values such as True, False, 'No phone service' and/or  'No internet service'.\n",
    "These values will be replaced with 'Yes' or 'No' as appropriate to ensure consistency.\n",
    "\n",
    "True will be replaced with 'Yes' because True means that the customers receive those services. While False, 'No phone service' and 'No internet service' will be replaced with 'No' because they mean that the customers do not (or cannot) receive those services. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace True with 'Yes' and replace False, 'No internet service' and 'No phone service' with 'No' in the train dataset.\n",
    "train_data = train_data.replace({\n",
    "    True: 'Yes',\n",
    "    False: 'No',\n",
    "    'No internet service': 'No',\n",
    "    'No phone service': 'No'\n",
    "}, inplace = False)\n",
    "\n",
    "# Confirm that the changes on the train dataset have been effected.\n",
    "train_data.columns\n",
    "for column in train_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, train_data[column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None is the Boolean representation of missing values. The columns with None are all categorical columns. The mode of these columns will be obtained and used to replace None."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace None on each column with the mode of the column.\n",
    "def replace_none_with_mode(train_data):\n",
    "   categorical_cols = train_data.select_dtypes(include='object').columns  # Select categorical columns\n",
    "\n",
    "   for col in categorical_cols:\n",
    "       mode_val = train_data[col].mode()[0]  # Calculate the mode of the column\n",
    "       train_data[col] = train_data[col].replace({None: mode_val})  # Replace None values with the mode\n",
    "\n",
    "   return train_data\n",
    "train_data = replace_none_with_mode(train_data)\n",
    "\n",
    "# Confirm that the changes on the train dataset have been effected.\n",
    "train_data.columns\n",
    "for column in train_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, train_data[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1003,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigate the columns on the test dataset.\n",
    "\n",
    "test_data.columns\n",
    "for column in test_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, test_data[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace True with 'Yes' and replace False, 'No internet service' and 'No phone service' with 'No' in the test dataset.\n",
    "test_data = test_data.replace({\n",
    "    True: 'Yes',\n",
    "    False: 'No',\n",
    "    'No internet service': 'No',\n",
    "    'No phone service': 'No'\n",
    "}, inplace = False)\n",
    "\n",
    "# Confirm that the changes on the test dataset have been effected.\n",
    "test_data.columns\n",
    "for column in test_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, test_data[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1005,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the customerID column from the train dataset since it has unique values that are non-beneficial to our modelling\n",
    "\n",
    "train_data.drop(columns='customerID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1006,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the customerID column from the test dataset since it has unique values that are non-beneficial to our modelling\n",
    "\n",
    "test_data.drop(columns='customerID', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1007,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values on the train dataset.\n",
    "\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset no longer has missing values. This is because None values (which is a boolean representation of missing values) has been changed to No."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1008,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the 'SeniorCitizen' column in the test dataset.\n",
    "\n",
    "test_data['SeniorCitizen'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'SeniorCitizen' column has numerical vales (0 and 1). These will be changed to 'No' and 'Yes' respectively in order to change the column datatype to an object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1009,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace 0 with 'No' and 1 with Yes in the 'SeniorCitizen' column of the test dataset.\n",
    "test_data['SeniorCitizen'] = test_data['SeniorCitizen'].replace({\n",
    "    0: 'No',\n",
    "    1: 'Yes'\n",
    "}, inplace = False)\n",
    "\n",
    "# Confirm that the changes on the test data has been effected\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1010,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values on the test dataset.\n",
    "\n",
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The test data has no missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1011,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total Charges column has an object datatype. This needs to be converted to float to enable us do some calculations on the column. The column will be investigated to see some of its features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1012,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the 'TotalCharges' column in the train dataset.\n",
    "\n",
    "train_data['TotalCharges'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1013,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the datatype of the 'TotalCharges' column in the train dataset to float by changing the contents to\n",
    "# numerical values.\n",
    "\n",
    "train_data['TotalCharges'] = pd.to_numeric(train_data['TotalCharges'], errors = 'coerce')\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'TotalCharges' column now has three missing values. These will be replaced with the mean value since it's now a numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1015,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the 'TotalCharges' column\n",
    "mean_value = train_data['TotalCharges'].mean()\n",
    "\n",
    "# Fill missing values with the mean value\n",
    "train_data['TotalCharges'] = train_data['TotalCharges'].fillna(mean_value)\n",
    "\n",
    "# Confirm that their are no missing values\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1016,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the 'TotalCharges' column in the test dataset.\n",
    "\n",
    "test_data['TotalCharges'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1017,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting the datatype of the 'TotalCharges' column in the test dataset to float by changing the contents to\n",
    "# numerical values.\n",
    "\n",
    "test_data['TotalCharges'] = pd.to_numeric(test_data['TotalCharges'], errors = 'coerce')\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1018,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the mean of the 'TotalCharges' column\n",
    "mean_value = test_data['TotalCharges'].mean()\n",
    "\n",
    "# Fill missing values with the mean value\n",
    "test_data['TotalCharges'] = test_data['TotalCharges'].fillna(mean_value)\n",
    "\n",
    "# Confirm that their are no missing values\n",
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the categorical and numerical columns of the train dataset\n",
    "train_cat = train_data.select_dtypes(include=['object']).columns\n",
    "train_num = train_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Obtain the categorical and numerical columns of the test dataset\n",
    "test_cat = test_data.select_dtypes(include=['object']).columns\n",
    "test_num = test_data.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the categorical values on the train dataset.\n",
    "\n",
    "train_data[train_cat].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the numerical values on the train dataset.\n",
    "\n",
    "train_data[train_num].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the correlation of the numerical values on the train dataset.\n",
    "\n",
    "train_data[train_num].corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing the correlation with a heatmap\n",
    "\n",
    "sns.heatmap(train_data[train_num].corr(), annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since there are calculations to be done on the 'Churn' column of the train dataset while answering the analytical questions, 'No' and 'Yes' values will be changed to 0 and 1 respectively in order to convert the column to a numerical column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing 'No' and 'Yes' to 0 and 1 respectively on the 'Churn' column of the train dataset.\n",
    "\n",
    "train_data['Churn'] = train_data['Churn'].replace(['No', 'Yes'], [0,1])\n",
    "train_data['Churn'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypothesis\n",
    "Null Hypothesis: There is no relationship between the monthly charges and the churn of customers.\n",
    "\n",
    "Alternate Hypothesis: There is a relationship between the monthly charges and the churn of customers.\n",
    "\n",
    "The hypothesis was tested using chi-square test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the null hypothesis.\n",
    "null_hypothesis = \"There is no relationship between the monthly charge and churn of customers.\"\n",
    "\n",
    "# Define the alternative hypothesis.\n",
    "alternative_hypothesis = \"There is a relationship between the monthly charge and churn of customers.\"\n",
    "\n",
    "# Perform the chi-square test\n",
    "observed = pd.crosstab(train_data['MonthlyCharges'], train_data['Churn'])\n",
    "\n",
    "chi2, p_value, _, _ = stats.chi2_contingency(observed)\n",
    "\n",
    "\n",
    "# Set the significance level\n",
    "alpha = 0.05\n",
    "\n",
    "\n",
    "# Print the test results\n",
    "print(\"Null Hypothesis:\", null_hypothesis)\n",
    "\n",
    "print(\"Alternative Hypothesis:\", alternative_hypothesis)\n",
    "\n",
    "print(\"Significance Level (alpha):\", alpha)\n",
    "\n",
    "print(\"Chi-square statistic:\", chi2)\n",
    "\n",
    "print(\"P-value:\", p_value)\n",
    "\n",
    "\n",
    "# Compare the p-value with the significance level\n",
    "if p_value < alpha:\n",
    "\n",
    "    print(\"Result: Reject the null hypothesis. There is a relationship between the monthly charges and churn of customers.\")\n",
    "\n",
    "else:\n",
    "\n",
    "    print(\"Result: Fail to reject the null hypothesis. There is no relationship between monthly charges and churn of customers.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering Questions with Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Analytical Questions\n",
    "1. What is the overall churn rate of the telecommunication company?\n",
    "2. What is the average monthly charges to churn customers compared to non-churn customers?\n",
    "3. What percentage of the top 100 most charged customers churned?\n",
    "4. What percentage of the top 100 least charged customers churned?\n",
    "5. What is the churn rate of male customers with partners, dependents and high monthly charges?\n",
    "6. What is the churn rate of customers without online security?\n",
    "7. What is the churn rate of customers without online backup?\n",
    "8. What is the churn rate of customers without device protection?\n",
    "9. What is the churn rate of customers without Tech support?\n",
    "10. How does the absence of online security, online backup, device protection and Tech support add up to lead to churning?\n",
    "11. How does the length of customers' contract affect their likelihood of churn?\n",
    "12. How does the length of customers' tenure affect their likelihood of churn?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 1\n",
    "What is the overall churn rate of the telecommunication company?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1027,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the churn rate\n",
    "total_customers = len(train_data)\n",
    "churned_customers = train_data['Churn'].sum()\n",
    "churn_rate = (churned_customers / total_customers) * 100\n",
    "\n",
    "# Display the churn rate\n",
    "print('Total Customers:', total_customers)\n",
    "print('Churned Customers:', churned_customers)\n",
    "print(f'Churn Rate: {churn_rate.round(1)}%')\n",
    "\n",
    "# Plot the churn rate\n",
    "plt.bar(['Churned', 'Not Churned'], [churn_rate, 100-churn_rate])\n",
    "plt.title('Overall Churn Rate Of The Telecommunication Network')\n",
    "plt.xlabel('Churn Status')\n",
    "plt.ylabel('Percentage')\n",
    "plt.ylim([0, 100])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 5043 customers initially out of which 1336 have churned, the telecommunication network has a churn rate of 26.5%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 2\n",
    "What is the average monthly charges to churn customers compared to non-churn customers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1028,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate churn and non-churn customers\n",
    "churn_customers = train_data[train_data['Churn'] == 1]\n",
    "non_churn_customers = train_data[train_data['Churn'] == 0]\n",
    "\n",
    "# Calculate the average monthly charges for churn and non-churn customers\n",
    "avg_churn_charges = churn_customers['MonthlyCharges'].mean()\n",
    "avg_non_churn_charges = non_churn_customers['MonthlyCharges'].mean()\n",
    "\n",
    "# Display the average charges\n",
    "print(f'Average Monthly Charges To Churn Customers: ${round(avg_churn_charges, 2)}')\n",
    "print(f'Average Monthly Charges To Non-Churn Customers: ${round(avg_non_churn_charges, 2)}')\n",
    "\n",
    "# Plot the average charges\n",
    "labels = ['Churn Customers', 'Non-Churn Customers']\n",
    "charges = [avg_churn_charges, avg_non_churn_charges]\n",
    "plt.bar(labels, charges)\n",
    "plt.ylabel('Average Monthly Charges ($)')\n",
    "plt.title('Average Monthly Charges To Churn Customers Compared to Non-Churn Customers')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the average monthly charges to churn customers is 75.21 dollars, higher than the average monthly charges to non-churn customers (61.44 dollars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 3\n",
    "What percentage of the top 100 most charged customers churned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1029,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by TotalCharges in descending order\n",
    "\n",
    "most_charged_data = train_data.sort_values(by='TotalCharges', ascending=False)\n",
    "\n",
    "# Select the top 100 most charged customers\n",
    "top_100_most_charged_customers = most_charged_data.head(100)\n",
    "\n",
    "# Count the number of churned customers among the top 100\n",
    "most_charged_churned_customers = top_100_most_charged_customers[top_100_most_charged_customers['Churn'] == 1]\n",
    "most_charged_churned_count = most_charged_churned_customers.shape[0]\n",
    "\n",
    "# Calculate the percentage of churned customers\n",
    "most_charged_percentage_churned = (most_charged_churned_count / 100) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f'The Percentage Of The Top 100 Most Charged Customers Who Churned: {most_charged_percentage_churned}%')\n",
    "\n",
    "# Create a pie chart to visualize the results\n",
    "labels = ['Churned Customers', 'Non-Churned Customers']\n",
    "sizes = [most_charged_percentage_churned, 100 - most_charged_percentage_churned]\n",
    "colors = ['#FF7F7F', '#7FB3FF']\n",
    "\n",
    "# Plot the top 100 most charged customers who churned\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.title('The Percentage Of The Top 100 Most Charged Customers Who Churned')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At an overall churn rate of 26.5%, only 7% of the top 100 most charged customers churned. This suggests that the total charges are not the only determinant of churn. There will be other factors that influenced the churn rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 4\n",
    "What percentage of the top 100 least charged customers churned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1030,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the DataFrame by TotalCharges in ascending order\n",
    "\n",
    "least_charged_data = train_data.sort_values(by='TotalCharges', ascending=True)\n",
    "\n",
    "# Select the top 100 least charged customers\n",
    "top_100_least_charged_customers = least_charged_data.head(100)\n",
    "\n",
    "# Count the number of churned customers among the top 100\n",
    "least_charged_churned_customers = top_100_least_charged_customers[top_100_least_charged_customers['Churn'] == 1]\n",
    "least_charged_churned_count = least_charged_churned_customers.shape[0]\n",
    "\n",
    "# Calculate the percentage of churned customers\n",
    "least_charged_percentage_churned = (least_charged_churned_count / 100) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f'The Percentage Of The Top 100 Least Charged Customers Who Churned: {least_charged_percentage_churned}%')\n",
    "\n",
    "# Create a pie chart to visualize the results\n",
    "labels = ['Churned Customers', 'Non-Churned Customers']\n",
    "sizes = [least_charged_percentage_churned, 100 - least_charged_percentage_churned]\n",
    "colors = ['#FF7F7F', '#7FB3FF']\n",
    "\n",
    "# Plot the top 100 most charged customers who churned\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.title('The Percentage Of The Top 100 Least Charged Customers Who Churned')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "33% of the top 100 least charged customers churned. This strenghtens the argument that the charges are not the only determinant of churn. A good number of those that got the least charges churned although the charges they paid were reasonably low."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 5\n",
    "What is the churn rate of male customers with partners, dependents and high monthly charges?\n",
    "\n",
    "N/B: It is assumed that high monthly charges refer to monthly charges equal to or above $100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1031,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the data to include only male customers with partners and dependents and high monthly charges\n",
    "filtered_train_data = train_data[(train_data['gender'] == 'Male') & (train_data['Partner'] == 'Yes') & (train_data['Dependents'] == 'Yes') & (train_data['MonthlyCharges'] > 100)]\n",
    "\n",
    "# Calculate the churn rate.\n",
    "rate_churned = filtered_train_data[filtered_train_data['Churn'] == 1].shape[0]/len(filtered_train_data) * 100\n",
    "\n",
    "# Calculate the non-churn rate.\n",
    "rate_non_churned = filtered_train_data[filtered_train_data['Churn'] == 0].shape[0]/len(filtered_train_data) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f'Churn Rate of Male Customers with Partners, Dependents and High Monthly Charges: {rate_churned}%')\n",
    "print(f'Non-Churn Rate of Male Customers with Partners, Dependents and High Monthly Charges: {rate_non_churned}%')\n",
    "\n",
    "# Create a bar plot to visualize the churn rate and non-churn rate.\n",
    "labels = ['Churned', 'Non-Churned']\n",
    "charges = [rate_churned, rate_non_churned]\n",
    "\n",
    "# Plot the churn rate of male customers with partners, dependents and high monthly charges\n",
    "plt.bar(labels, charges)\n",
    "plt.xlabel('Churn Status')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "plt.title('Churn Rate Of Male Customers With Partners, Dependents \\nAnd High Monthly Charges')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the assumption that male customers with partners and dependents have higher financial demands to meet under average conditions, this question analyzed the rate at which high charges influenced the churn rate of this set of customers. It was discovered that only 20% of male customers with partners, dependents and high monthly charges churned. This confirms that the charges are not the only determinant of churn. There are other factors that greatly influenced the churn rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 6\n",
    "What is the churn rate of customers without online security?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1032,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only customers without online security\n",
    "no_security_customers = train_data[train_data['OnlineSecurity'] == 'No']\n",
    "\n",
    "# Calculate the churn rate of customers without online security\n",
    "churned_customers = no_security_customers[no_security_customers['Churn'] == 1]\n",
    "churned_rate = (churned_customers.shape[0] / no_security_customers.shape[0]) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f'Churn Rate Of Customers Without Online Security: {churned_rate}%')\n",
    "\n",
    "# Create a pie chart to visualize the results\n",
    "labels = ['Churned', 'Non-Churned']\n",
    "sizes = [churned_rate, 100 - churned_rate]\n",
    "colors = ['#FF7F7F', '#7FB3FF']\n",
    "\n",
    "# Plot the churn rate of customers without online security\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('Churn Rate Of Customers Without Online Security')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.3% of customers without online security churned. This means that the absence of online security influenced the churn rate of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 7\n",
    "What is the churn rate of customers without online backup?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1033,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only customers without online backup\n",
    "no_backup_customers = train_data[train_data['OnlineBackup'] == 'No']\n",
    "\n",
    "# Calculate the churn rate of customers without online backup\n",
    "churned_customers = no_backup_customers[no_backup_customers['Churn'] == 1]\n",
    "churned_rate = (churned_customers.shape[0] / no_backup_customers.shape[0]) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f'Churn Rate Of Customers Without Online Backup: {churned_rate}%')\n",
    "\n",
    "# Create a pie chart to visualize the results\n",
    "labels = ['Churned', 'Non-Churned']\n",
    "sizes = [churned_rate, 100 - churned_rate]\n",
    "colors = ['#FF7F7F', '#7FB3FF']\n",
    "\n",
    "# Plot the churn rate of customers without online backup\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('Churn Rate Of Customers Without Online Backup')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29.2% of customers without online backup churned. This means that the absence of online backup influenced the churn rate of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 8\n",
    "What is the churn rate of customers without device protection?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1034,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only customers without device protection\n",
    "no_device_protection_customers = train_data[train_data['DeviceProtection'] == 'No']\n",
    "\n",
    "# Calculate the churn rate of customers without device protection\n",
    "churned_customers = no_device_protection_customers[no_device_protection_customers['Churn'] == 1]\n",
    "churned_rate = (churned_customers.shape[0] / no_device_protection_customers.shape[0]) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f'Churn Rate Of Customers Without Device Protection: {churned_rate}%')\n",
    "\n",
    "# Create a pie chart to visualize the results\n",
    "labels = ['Churned', 'Non-Churned']\n",
    "sizes = [churned_rate, 100 - churned_rate]\n",
    "colors = ['#FF7F7F', '#7FB3FF']\n",
    "\n",
    "# Plot the churn rate of customers without device protection\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('Churn Rate Of Customers Without Device Protection')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28.6% of customers without device protection churned. This means that the absence of device protection influenced the churn rate of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 9\n",
    "What is the churn rate of customers without Tech support?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the DataFrame to include only customers without Tech support\n",
    "no_tech_support_customers = train_data[train_data['TechSupport'] == 'No']\n",
    "\n",
    "# Calculate the churn rate of customers without Tech support\n",
    "churned_customers = no_tech_support_customers[no_tech_support_customers['Churn'] == 1]\n",
    "churned_rate = (churned_customers.shape[0] / no_tech_support_customers.shape[0]) * 100\n",
    "\n",
    "# Display the result\n",
    "print(f'Churn Rate Of Customers Without Tech Support: {churned_rate}%')\n",
    "\n",
    "# Create a pie chart to visualize the result\n",
    "labels = ['Churned', 'Non-Churned']\n",
    "sizes = [churned_rate, 100 - churned_rate]\n",
    "colors = ['#FF7F7F', '#7FB3FF']\n",
    "\n",
    "# Plot the churn rate of customers without Tech support\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')\n",
    "plt.title('Churn Rate Of Customers Without Tech Support')\n",
    "plt.legend(loc=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31.4% of customers without Tech support churned. This means that the absence of Tech support influenced the churn rate of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 10\n",
    "How does the absence of online security, online backup, device protection and Tech support add up to lead to churning?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1036,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of churned customers based on absence of each feature\n",
    "churned_customers = train_data[train_data['Churn'] == 1]\n",
    "\n",
    "absence_of_security = churned_customers[churned_customers['OnlineSecurity'] == 'No'].shape[0]\n",
    "absence_of_backup = churned_customers[churned_customers['OnlineBackup'] == 'No'].shape[0]\n",
    "absence_of_protection = churned_customers[churned_customers['DeviceProtection'] == 'No'].shape[0]\n",
    "absence_of_tech_support = churned_customers[churned_customers['TechSupport'] == 'No'].shape[0]\n",
    "\n",
    "# Calculate the rate at which the absence of each feature contributes to churning\n",
    "total_churned_customers = churned_customers.shape[0]\n",
    "\n",
    "rate_of_security = (absence_of_security / total_churned_customers) * 100\n",
    "rate_of_backup = (absence_of_backup / total_churned_customers) * 100\n",
    "rate_of_protection = (absence_of_protection / total_churned_customers) * 100\n",
    "rate_of_tech_support = (absence_of_tech_support / total_churned_customers) * 100\n",
    "\n",
    "# Display the results\n",
    "print(f'Churn Percentage Of Customers Without Online Security: {rate_of_security}%')\n",
    "print(f'Churn Percentage Of Customers Without Online Backup: {rate_of_backup}%')\n",
    "print(f'Churn Percentage Of Customers Without Device Protection: {rate_of_protection}%')\n",
    "print(f'Churn Percentage Of Customers Without Tech Support: {rate_of_tech_support}%')\n",
    "\n",
    "# Create a bar plot to visualize the results\n",
    "labels = ['No Online Security', 'No Online Backup', 'No Device Protection', 'No Tech Support']\n",
    "rates = [rate_of_security, rate_of_backup, rate_of_protection, rate_of_tech_support]\n",
    "\n",
    "# Plot the rate at which the absence of each feature contributes to churning\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=labels, y=rates, palette='viridis')\n",
    "plt.title('Churn Rate Due To Absence Of Features')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Rate (%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that out of all the churned customers, 84.0% did not have online security, 72.4% did not have online backup, 70.6% did not have device protection, and 83.5% did not have Tech support. These factors are majorly responsible for churning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 11\n",
    "How does the length of customers' contract affect their likelihood of churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1037,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate churn rates for different contract lengths\n",
    "contract_lengths = train_data['Contract'].unique()\n",
    "churn_rates = []\n",
    "\n",
    "for length in contract_lengths:\n",
    "   churn_rate = train_data[train_data['Contract'] == length]['Churn'].value_counts(normalize=True).get(1, 0) * 100\n",
    "   churn_rates.append(churn_rate)\n",
    "\n",
    "# Display the contract lenghts and churn rates\n",
    "print(f'Contract Lengths: {contract_lengths}')\n",
    "print(f'Churn Rates: {churn_rates}')\n",
    "\n",
    "# Create a bar plot to visualize the churn rates by contract length\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(contract_lengths, churn_rates)\n",
    "plt.xlabel('Contract Length')\n",
    "plt.ylabel('Churn Rate (%)')\n",
    "plt.title('Churn Rates by Contract Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the length of a customer's contract affects the churn rate. Customers with month-to-month contract have a high churn rate of 43.1%. This is followed by customers with one year contract length with a churn rate of 11.6%. While customers with two year contract length have a low churn rate of 2.4%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question 12\n",
    "How does the length of customers' tenure affect their likelihood of churn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1038,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate churn rates for different tenure lengths\n",
    "tenure_lengths = train_data['tenure'].unique()\n",
    "churn_rates = []\n",
    "\n",
    "for length in tenure_lengths:\n",
    "   churn_rate = train_data[train_data['tenure'] == length]['Churn'].value_counts(normalize=True).get(1, 0)\n",
    "   churn_rates.append(churn_rate)\n",
    "\n",
    "# Create a bar plot to visualize the churn rates by tenure length\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.bar(tenure_lengths, churn_rates)\n",
    "plt.xlabel('Tenure Length (Number of Months)')\n",
    "plt.ylabel('Churn Rate')\n",
    "plt.title('Churn Rates by Tenure Length')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown, the more the tenure length (number of months) a customer stays with the telecommunication company, the lower the likelihood of churn. This is ralated to the result of the likelihood of churn based on the contract length of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommendation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1039,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1040,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Pandas Profiling\n",
    "# # TRAIN\n",
    "\n",
    "# profile = ProfileReport(train_data, title = \"Train Dataset\", html = {'style': {full_width: True}})\n",
    "# profile.to_notebook_iframe()\n",
    "# profile.to_file('(Trainset) Pandas-Profiling_Report.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1041,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create a copy of the train and test datasets on which to carry out the feature engineering processes\n",
    "\n",
    "train_data_transformed = train_data.copy()\n",
    "test_data_transformed = test_data.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1042,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a MinMaxScaler object\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "# Define the columns to scale\n",
    "columns_to_scale = ['tenure', 'MonthlyCharges', 'TotalCharges']\n",
    "\n",
    "# Use MinMaxScaler to scale the numerical columns on the train dataset\n",
    "train_data_transformed[columns_to_scale] = scaler.fit_transform(train_data_transformed[columns_to_scale])\n",
    "train_data_transformed = pd.DataFrame(train_data_transformed, columns=train_data.columns)\n",
    "\n",
    "# View the scaled data\n",
    "train_data_transformed[columns_to_scale].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1043,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use MinMaxScaler to scale the numerical columns on the test dataset\n",
    "test_data_transformed[columns_to_scale] = scaler.fit_transform(test_data_transformed[columns_to_scale])\n",
    "test_data_transformed = pd.DataFrame(test_data_transformed, columns=test_data.columns)\n",
    "\n",
    "# View the scaled data\n",
    "test_data_transformed[columns_to_scale].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the values of the churn column in the train dataset were changed from 'Yes' and 'No' to 1 and 0 respectively to aid the calculations used to answer the analytical questions, and scaling has been applied to the numerical columns of both train and test datasets, it is important to relist the numerical and categorical columns of both datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1044,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain the categorical and numerical columns of the train dataset\n",
    "train_cat = train_data.select_dtypes(include=['object']).columns\n",
    "train_num = train_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Obtain the categorical and numerical columns of the test dataset\n",
    "test_cat = test_data.select_dtypes(include=['object']).columns\n",
    "test_num = test_data.select_dtypes(include=['float64', 'int64']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1045,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an encoder object using OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse=False, drop=\"first\")\n",
    "\n",
    "# Create seperate DataFrames for categorical columns and numerical columns for the train dataset\n",
    "train_cat_df = train_data_transformed[train_cat]\n",
    "train_num_df = train_data_transformed[train_num]\n",
    "\n",
    "\n",
    "# Use OneHotEncoder to encode the categorical columns on the train dataset\n",
    "encoder.fit(train_cat_df)\n",
    "train_encoded = encoder.transform(train_cat_df).tolist()\n",
    "train_encoded_data = pd.DataFrame(train_encoded, columns=encoder.get_feature_names_out())\n",
    "train_encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1046,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the train_encoded_data and the train_num_df to have the train\n",
    "\n",
    "train = pd.concat([train_encoded_data, train_num_df.set_axis(train_encoded_data.index)], axis=1)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1047,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create seperate DataFrames for categorical columns and numerical columns for the test dataset\n",
    "test_cat_df = test_data_transformed[test_cat]\n",
    "test_num_df = test_data_transformed[test_num]\n",
    "\n",
    "\n",
    "# Use OneHotEncoder to encode the categorical columns on the test dataset\n",
    "encoder.fit(test_cat_df)\n",
    "test_encoded = encoder.transform(test_cat_df).tolist()\n",
    "test_encoded_data = pd.DataFrame(test_encoded, columns=encoder.get_feature_names_out())\n",
    "test_encoded_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1048,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate the test_encoded_data and the test_num_df to have the test\n",
    "\n",
    "test = pd.concat([test_encoded_data, test_num_df.set_axis(test_encoded_data.index)], axis=1)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1049,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop('Churn', axis=1)\n",
    "y = train['Churn']\n",
    "\n",
    "# Split train dataset into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Print the shape of the train dataset\n",
    "print(\"Train set shape:\", X_train.shape, y_train.shape, X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balancing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1050,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform oversampling using SMOTE\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Perform undersampling using RandomUnderSampler\n",
    "rus = RandomUnderSampler(random_state=42)\n",
    "X_train_resampled, y_train_resampled = rus.fit_resample(X_train, y_train)\n",
    "\n",
    "# Print the class distribution before and after balancing\n",
    "print(\"Before balancing:\")\n",
    "print(y_train.value_counts())\n",
    "\n",
    "print(\"After balancing:\")\n",
    "print(pd.Series(y_train_resampled).value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1051,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of models to train and evaluate\n",
    "models = [\n",
    "    ('Logistic Regression', LogisticRegression(random_state=42, solver='liblinear', max_iter=1000)),\n",
    "    ('Decision Tree', DecisionTreeClassifier(random_state=42, criterion='gini', min_samples_leaf=8, max_depth=5)),\n",
    "    ('Random Forest', RandomForestClassifier(random_state=42, n_estimators=100, max_depth=5)),\n",
    "    ('Gradient Boosting', GradientBoostingClassifier(random_state=42, n_estimators=100, learning_rate=0.1)),\n",
    "    ('Support Vector Machine', SVC(random_state=42, kernel='rbf', C=1.0)),\n",
    "    ('Gaussian Naive Bayes', GaussianNB()),\n",
    "    ('K-Nearest Neighbors', KNeighborsClassifier(n_neighbors=5)),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and evaluation with unbalanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1052,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the performance metrics of unbalanced dataset\n",
    "unbal_performance_metrics = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC_AUC'])\n",
    "\n",
    "# Model training, evaluation and result calculation\n",
    "for model_name, model in models:\n",
    "    # Model training with unbalanced dataset\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Using models to make predictions on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate performance metrics of unbalanced dataset\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    \n",
    "    # Store the calculation results in the unbalanced performance metrics DataFrame\n",
    "    unbal_performance_metrics = unbal_performance_metrics._append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Print the performance metrics DataFrame\n",
    "print('The performance metrics of the unbalanced dataset')\n",
    "unbal_performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the f1 score of the models, Guassian Naive Bayes is the best model for the unbalanced dataset with an f1 score of 0.617647."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and evaluation with balanced dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1053,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame to store the performance metrics of balanced dataset\n",
    "bal_performance_metrics = pd.DataFrame(columns=['Model', 'Accuracy', 'Precision', 'Recall', 'F1 Score', 'ROC_AUC'])\n",
    "\n",
    "# Model training, evaluation and result calculation\n",
    "for model_name, model in models:\n",
    "    # Model training with balanced dataset\n",
    "    model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Using models to make predictions on validation set\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    # Calculate performance metrics of balanced dataset\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    roc_auc = roc_auc_score(y_val, y_pred)\n",
    "    \n",
    "    # Store the calculation results in the balanced performance metrics DataFrame\n",
    "    bal_performance_metrics = bal_performance_metrics._append({\n",
    "        'Model': model_name,\n",
    "        'Accuracy': accuracy,\n",
    "        'Precision': precision,\n",
    "        'Recall': recall,\n",
    "        'F1 Score': f1,\n",
    "        'ROC_AUC': roc_auc\n",
    "    }, ignore_index=True)\n",
    "\n",
    "# Print the performance metrics DataFrame\n",
    "print('The performance metrics of the balanced dataset')\n",
    "bal_performance_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the f1 score of the models, Random Forest is the best model for the balanced dataset with an f1 score of 0.630339."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper-parameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
