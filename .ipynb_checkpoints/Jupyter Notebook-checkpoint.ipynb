{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TELCO CHURN ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis\n",
    "\n",
    "Null Hypothesis: There is no relationship between the monthly charges and the churn of customers.\n",
    "\n",
    "Alternate Hypothesis: There is a relationship between the monthly charges and the churn of customers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analytical Questions\n",
    "\n",
    "1. What is the overall churn rate of the telecommunication network?\n",
    "2. What is the average monthly spending of churn customers compared to non-churn customers?\n",
    "3. What percentage of the top 100 most charged customers churned?\n",
    "4. What is the rate of churning of customers without online security?\n",
    "5. What is the rate of churning of customers without online backup?\n",
    "6. What is the rate of churning of customers without device protection?\n",
    "7. What is the rate of churning of customers without Tech support?\n",
    "8. How does the absence of online security, online backup, device protection and Tech support add up to lead to churning?\n",
    "9. Does the length of a customer's contract affect their likelihood of churn?\n",
    "10. How does the customer's tenure affect their likelihood of churn?\n",
    "11. How many male customers who have partners and dependents with high monthly charges churned?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the needed packages.\n",
    "\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are three datasets for this analysis which are located in three different places. The first dataset can be found on a SQL server and is to be accessed remotely. The second dataset can be found as an excel file on One Drive. The access link was provided and used to access and download the dataset. While the third dataset can be found as a csv file on a Github repository whose link was provided as well and used to clone the dataset into the local machine.\n",
    "\n",
    "The first and last datasets have been identified as the train datasets, while the second dataset has been identifed as the test dataset. The train datasets will be assessed and merged together, and used to build models independently. While the test dataset will be used to test the models independently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the first dataset from SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the server instance variable such as the server you are connecting to, database , username and password.\n",
    "server = 'dap-projects-database.database.windows.net'\n",
    "database = 'dapDB'\n",
    "username = 'dataAnalyst_LP2'\n",
    "password = 'A3g@3kR$2y'\n",
    "\n",
    "# The connection string is an f string that includes all the variable above to extablish a connection to the server.\n",
    "connection_string = f\"DRIVER={{SQL Server}};SERVER={server};DATABASE={database};UID={username};PWD={password}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "Error",
     "evalue": "('HY000', '[HY000] [Microsoft][ODBC SQL Server Driver][SQL Server]Reason: Login failed due to client TLS version being less than minimal TLS version allowed by the server. (47072) (SQLDriverConnect); [HY000] [Microsoft][ODBC SQL Server Driver][SQL Server]Reason: Login failed due to client TLS version being less than minimal TLS version allowed by the server. (47072)')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[64], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Using the connect method of the pyodbc library to pass in the connection string.\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# N/B: This will connect to the server and might take a few seconds to be complete.\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Check your internet connection if it takes more time than necessary.\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[43mpyodbc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconnection_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mError\u001b[0m: ('HY000', '[HY000] [Microsoft][ODBC SQL Server Driver][SQL Server]Reason: Login failed due to client TLS version being less than minimal TLS version allowed by the server. (47072) (SQLDriverConnect); [HY000] [Microsoft][ODBC SQL Server Driver][SQL Server]Reason: Login failed due to client TLS version being less than minimal TLS version allowed by the server. (47072)')"
     ]
    }
   ],
   "source": [
    "# Using the connect method of the pyodbc library to pass in the connection string.\n",
    "\n",
    "# N/B: This will connect to the server and might take a few seconds to be complete.\n",
    "# Check your internet connection if it takes more time than necessary.\n",
    "\n",
    "connection = pyodbc.connect(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The SQL query used to get the dataset is shown below. Note that you will not have permissions to insert delete\n",
    "# or update this database table. dbo.LP2_Telco_churn_first_3000 is the name of the table. The dbo in front of the name \n",
    "# is a naming convention in Microsoft SQL Server.\n",
    "\n",
    "query = \"Select * from dbo.LP2_Telco_churn_first_3000\"\n",
    "df1 = pd.read_sql(query, connection)\n",
    "\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the last dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the last dataset.\n",
    "\n",
    "df3 = pd.read_csv('LP2_Telco-churn-last-2000.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the first five rows of the last dataset.\n",
    "\n",
    "df3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the columns of the first dataset.\n",
    "\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the columns of the last dataset.\n",
    "\n",
    "df3.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since both datasets have the same column names and index number, they can be concatenated to have the train dataset.\n",
    "\n",
    "train_data = pd.concat([df1, df3])\n",
    "\n",
    "# Saving the train dataset to a new csv file.\n",
    "train_data.to_csv('Train-Data.csv')\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accessing the second dataset (test dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the test dataset.\n",
    "\n",
    "test_data = pd.read_excel('Telco-churn-second-2000.xlsx')\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of rows and columns on the train dataset.\n",
    "\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the number of rows and columns on the test dataset.\n",
    "\n",
    "test_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train dataset has 5043 columns and 21 columns while the test dataset has 2000 rows and 20 columns. Let's identify the column in the train dataset that is absent in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the columns of the train dataset.\n",
    "\n",
    "train_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspecting the columns of the test dataset.\n",
    "\n",
    "test_data.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be seen, the test dataset has the same columns as the train dataset with the exception of the churn column which can be found on the train dataset alone. This is understandable as the churn column on the train dataset provides information on whether customers churn or  not, which is used to build the best ML model. This column is not needed on the test dataset, rather the model built is tested on the test dataset to check the ability of the model to predict whether a customet will churn or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the statistical data on the train dataset.\n",
    "\n",
    "train_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the statistical data on the test dataset.\n",
    "\n",
    "test_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatypes and the presence of missing values on the train dataset.\n",
    "\n",
    "train_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the datatypes and the presence of missing values on the test dataset.\n",
    "\n",
    "test_data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the datatype of the 'SeniorCitizen' column is an object on the train data but an integer on the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming the number of cells with missing values on each column of the train dataset.\n",
    "\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 'MultipleLines' column has 269 missing values. The 'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV' and 'StreamingMovies' columns all have 651 missing values. This needs to be evaluated further to find out if these missing values are exactly on the same rows. The 'TotalCharges' column has 5 missing values while the 'Churn' column has 1 missing value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming that there are no missing values on the test dataset.\n",
    "\n",
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no missing values on the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the presence of duplicates on the train dataset.\n",
    "\n",
    "train_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows on the train dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for the presence of duplicates on the test dataset.\n",
    "\n",
    "test_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are no duplicate rows on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the columns on the train dataset.\n",
    "\n",
    "train_data.columns\n",
    "for column in train_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, train_data[column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing True with 'Yes' and replacing False, 'None', 'No internet service' and 'No phone service' with 'No'\n",
    "# in the train dataset.\n",
    "\n",
    "train_data = train_data.replace({\n",
    "    True: 'Yes',\n",
    "    False: 'No',\n",
    "    None: 'No',\n",
    "    'No internet service': 'No',\n",
    "    'No phone service': 'No'\n",
    "}, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming that the changes have been effected.\n",
    "\n",
    "train_data.columns\n",
    "for column in train_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, train_data[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values.\n",
    "\n",
    "train_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Investigating the columns on the test dataset.\n",
    "\n",
    "test_data.columns\n",
    "for column in test_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, test_data[column].unique()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replacing 1 with 'Yes' and replacing 0, 'No internet service' and 'No phone service' with 'No' in the test dataset.\n",
    "\n",
    "test_data = test_data.replace({\n",
    "    1: 'Yes',\n",
    "    0: 'No',\n",
    "    'No internet service': 'No',\n",
    "    'No phone service': 'No'\n",
    "}, inplace = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirming that the changes have been effected.\n",
    "\n",
    "test_data.columns\n",
    "for column in test_data.columns:\n",
    "    print('column: {} - unique value: {}'.format(column, test_data[column].unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking for missing values.\n",
    "\n",
    "test_data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markdown\n",
    "\n",
    "Total Charges"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
